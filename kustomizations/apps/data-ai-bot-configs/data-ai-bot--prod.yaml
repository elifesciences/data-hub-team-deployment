toolDefinitions:
  fromPythonToolInstance:
    - name: get_joke
      module: data_ai_bot.tools.example.joke
      key: get_joke
      description: |-
        Fetches a random programming joke.

  fromPythonToolClass:
    - name: how_to_use_this_tool
      description: |-
        Explain to the user how to use this AI tool.
        This also summarised what this tool knows.
        Also provides some example prompts.
        Only use this tool if the user asked for it.
      module: data_ai_bot.tools.sources.static
      className: StaticContentTool
      initParameters:
        content: |-
          You can for example ask questions relating to individual DocMap by a manuscript id.

          Example prompts:
            - From the DocMap, who are the editors of 101859?
            - How many reviews does the DocMap of 101859 have?

    - name: get_zen_quote
      module: data_ai_bot.tools.sources.web_api
      className: WebApiTool
      initParameters:
        description: |-
          Retrieve a random inspirational zen quote as JSON.
        url: https://zenquotes.io/api/random

    - name: get_docmap_by_manuscript_id
      description: |-
        Fetches a DocMap as JSON from the Data Hub DocMaps API.
      module: data_ai_bot.tools.sources.web_api
      className: WebApiTool
      initParameters:
        inputs:
          manuscript_id:
            type: string
            description: 'The 5 or 6 digit eLife manuscript id'
            regex: '\d{5,6}'
        url: 'https://data-hub-api.elifesciences.org/enhanced-preprints/docmaps/v2/by-publisher/elife/get-by-manuscript-id'
        query_parameters:
          manuscript_id: '{{ manuscript_id }}'

    - name: search_elife_papers
      description: |-
        Search eLife papers and returns JSON. This also returns the total count.
      module: data_ai_bot.tools.sources.web_api
      className: WebApiTool
      initParameters:
        inputs:
          query:
            type: string
            description: 'The search query. Can be empty.'
          article_type:
            type: string
            description: |-
              The article type filter criteria or empty.
              Only use if specified by user.
              One of: correction,editorial,expression-concern,feature,insight,research-advance,research-article,research-communication,retraction,registered-report,replication-study,review-article,scientific-correspondence,short-report,tools-resources
          subject_area:
            type: string
            description: |-
              The subject area filter criteria or empty.
              Only use if specified by user.
              One of: biochemistry-chemical-biology,cancer-biology,cell-biology,chromosomes-gene-expression,computational-systems-biology,developmental-biology,ecology,epidemiology-global-health,evolutionary-biology,genetics-genomics,immunology-inflammation,medicine,microbiology-infectious-disease,neuroscience,physics-living-systems,plant-biology,stem-cells-regenerative-medicine,structural-biology-molecular-biophysics
          significance:
            type: string
            description: |-
              The minimum significance filter criteria or empty.
              Only use if specified by user.
              One of: landmark,fundamental,important,valuable,useful
          strength:
            type: string
            description: |-
              The minimum strength filter criteria or empty.
              Only use if specified by user.
              One of: exceptional,compelling,convincing,solid,incomplete,inadequate
          start_date:
            type: string
            description: 'The start date to filter by, in YYYY-MM-DD format.'
            regex: '\d{4}-\d{2}-\d{2}|'
          end_date:
            type: string
            description: 'The end date to filter by, in YYYY-MM-DD format.'
            regex: '\d{4}-\d{2}-\d{2}|'
        url: 'https://api.elifesciences.org/search'
        query_parameters:
          for: '{{ query }}'
          type[]: '{{ article_type }}'
          subject[]: '{{ subject_area }}'
          elifeAssessmentSignificance[]: '{{ significance }}'
          elifeAssessmentStrength[]: '{{ strength }}'
          'start-date': '{{ start_date }}'
          'end-date': '{{ end_date }}'

    - name: get_senior_editors
      description: |-
        Retrieves a list of senior editors along with their expertise.
        Only use this tool if instructed to find senior editors by expertise.
        Returns JSON.
      module: data_ai_bot.tools.sources.bigquery
      className: BigQueryTool
      initParameters:
        project_name: elife-data-pipeline
        sql_query: |-
          SELECT
            Name,
            Keywords,
            Techniques,
            Subject_Areas,
            Research_Interests,
            Website_URL,
            Current_Availability,
            Role_Name
          FROM `elife-data-pipeline.prod.mv_Editorial_Public_Editor_Profile`
          WHERE Role_Name = 'Senior Editor'

    - name: get_data_hub_bigquery_views_pipeline_log
      description: |-
        Retrieves a the latest list of BigQuery materialized views log.
        This includes the number of bytes billed that are used to calcuate the cost.
      module: data_ai_bot.tools.sources.bigquery
      className: BigQueryTool
      initParameters:
        project_name: elife-data-pipeline
        sql_query: |-
          SELECT
            source_view_name,
            destination_table_name,
            STRUCT(
              (slot_millis / 1000) AS slot_time_secs,
              total_bytes_billed,
              (total_bytes_billed * 6.25 / (1024 * 1024 * 1024 * 1024)) AS cost_in_usd  -- $6.25 per TiB
            ) AS latest_run,
            (
              SELECT AS STRUCT
                COUNT(*) AS runs,
                SUM(total_bytes_billed) AS total_bytes_billed,
                (SUM(total_bytes_billed) * 6.25 / (1024 * 1024 * 1024 * 1024)) AS cost_in_usd
              FROM `elife-data-pipeline.prod.data_hub_bigquery_views_pipeline_log` AS count_log
              WHERE count_log.destination_dataset = log.destination_dataset
                AND count_log.source_view_name = log.source_view_name
                AND count_log.destination_table_name = log.destination_table_name
                AND DATE(count_log.data_hub_imported_timestamp) < CURRENT_DATE()
                AND DATE(count_log.data_hub_imported_timestamp) >= DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY)
            ) AS previous_7days
          FROM `elife-data-pipeline.prod.data_hub_bigquery_views_pipeline_log` AS log
          WHERE destination_dataset = 'prod'
            AND data_hub_imported_timestamp = (
              SELECT MAX(data_hub_imported_timestamp)
              FROM `elife-data-pipeline.prod.data_hub_bigquery_views_pipeline_log`
              WHERE destination_dataset = 'prod'
            )
          ORDER BY total_bytes_billed DESC

toolCollectionDefinitions:
  fromMcp:
    - name: data_hub_mcp
      url: 'http://data-ai-mcp-api--prod:8080/mcp/'
      transport: streamable-http
      tools:
        - get_rp_paper_submissions
    - name: data_ai_other_mcp
      url: 'http://data-ai-mcp-api--prod:8080/mcp/'
      transport: streamable-http
      tools:
        - get_joke_via_web_api
        - search_ecr
        - search_reviewer

models:
  - model_name: gpt-5-mini
    base_url: https://api.openai.com/v1
    api_key: "{{ env['OPENAI_API_KEY'] }}"
  - model_name: gpt-5
    base_url: https://api.openai.com/v1
    api_key: "{{ env['OPENAI_API_KEY'] }}"

managedAgents:
  - name: data_hub_agent
    description: |-
      Use this agent to retrieve data from Data Hub.
      Such as Reviewed Preprint (RP) submission information.
    systemPrompt: |-
      You are Data Hub Agent, the BigQuery SQL expert.
      Ensure to answer accurately.
      Ask follow up question if unsure.
      The final answer should be formatted as simple Markdown (without unnecessary headings).
    toolCollections:
      - data_hub_mcp
    model: gpt-5

agent:
  systemPrompt: |-
    You are Data AI Bot.
    Ensure to answer accurately.
    Ask follow up question if unsure.
    The final answer should be formatted as simple Markdown (without unnecessary headings).

    When a tool returns an error, empty result, or null:
    - NEVER substitute information from your training data
    - Explicitly inform the user that the tool encountered an error
    - State what information is unavailable
    - Do NOT provide any data that wasn't returned by the tool
  tools:
    - get_docmap_by_manuscript_id
    - how_to_use_this_tool
    - get_zen_quote
    - search_elife_papers
    - get_senior_editors
    - get_data_hub_bigquery_views_pipeline_log
  toolCollections:
    - data_ai_other_mcp
  managedAgents:
    - data_hub_agent
  model: gpt-5-mini
